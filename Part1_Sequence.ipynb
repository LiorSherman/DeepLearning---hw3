{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 1: Sequence Models\n",
    "<a id=part1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will learn about working with text sequences using recurrent neural networks.\n",
    "We'll go from a raw text file all the way to a fully trained GRU-RNN model and generate works of art!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Using device: cpu\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "test = unittest.TestCase()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation with a char-level RNN\n",
    "<a id=part1_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the corpus\n",
    "<a id=part1_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by downloading a corpus containing all the works of William Shakespeare.\n",
    "Since he was very prolific, this corpus is fairly large and will provide us with enough data for\n",
    "obtaining impressive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Corpus file C:\\Users\\liorsher\\.pytorch-datasets\\shakespeare.txt exists, skipping download.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "CORPUS_URL = 'https://github.com/cedricdeboom/character-level-rnn-datasets/raw/master/datasets/shakespeare.txt'\n",
    "DATA_DIR = pathlib.Path.home().joinpath('.pytorch-datasets')\n",
    "\n",
    "def download_corpus(out_path=DATA_DIR, url=CORPUS_URL, force=False):\n",
    "    pathlib.Path(out_path).mkdir(exist_ok=True)\n",
    "    out_filename = os.path.join(out_path, os.path.basename(url))\n",
    "    \n",
    "    if os.path.isfile(out_filename) and not force:\n",
    "        print(f'Corpus file {out_filename} exists, skipping download.')\n",
    "    else:\n",
    "        print(f'Downloading {url}...')\n",
    "        with urllib.request.urlopen(url) as response, open(out_filename, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)\n",
    "        print(f'Saved to {out_filename}.')\n",
    "    return out_filename\n",
    "    \n",
    "corpus_path = download_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the text into memory and print a snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Corpus length: 6347703 chars\n",
      "ALLS WELL THAT ENDS WELL\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "Dramatis Personae\n",
      "\n",
      "  KING OF FRANCE\n",
      "  THE DUKE OF FLORENCE\n",
      "  BERTRAM, Count of Rousillon\n",
      "  LAFEU, an old lord\n",
      "  PAROLLES, a follower of Bertram\n",
      "  TWO FRENCH LORDS, serving with Bertram\n",
      "\n",
      "  STEWARD, Servant to the Countess of Rousillon\n",
      "  LAVACHE, a clown and Servant to the Countess of Rousillon\n",
      "  A PAGE, Servant to the Countess of Rousillon\n",
      "\n",
      "  COUNTESS OF ROUSILLON, mother to Bertram\n",
      "  HELENA, a gentlewoman protected by the Countess\n",
      "  A WIDOW OF FLORENCE.\n",
      "  DIANA, daughter to the Widow\n",
      "\n",
      "  VIOLENTA, neighbour and friend to the Widow\n",
      "  MARIANA, neighbour and friend to the Widow\n",
      "\n",
      "  Lords, Officers, Soldiers, etc., French and Florentine  \n",
      "\n",
      "SCENE:\n",
      "Rousillon; Paris; Florence; Marseilles\n",
      "\n",
      "ACT I. SCENE 1.\n",
      "Rousillon. The COUNT'S palace\n",
      "\n",
      "Enter BERTRAM, the COUNTESS OF ROUSILLON, HELENA, and LAFEU, all in black\n",
      "\n",
      "  COUNTESS. In delivering my son from me, I bury a second husband.\n",
      "  BERTRAM. And I in going, madam, weep o'er my father's death anew;\n",
      "    but I must attend his Majesty's command, to whom I am now in\n",
      "    ward, evermore in subjection.\n",
      "  LAFEU. You shall find of the King a husband, madam; you, sir, a\n",
      "    father. He that so generally is at all times good must of\n",
      "    \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "print(f'Corpus length: {len(corpus)} chars')\n",
    "print(corpus[7:1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "<a id=part1_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'll need is to map from each unique character in the corpus to an index that will represent it in our learning process.\n",
    "\n",
    "**TODO**: Implement the `char_maps()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, ',': 9, '-': 10, '.': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, '[': 52, ']': 53, '_': 54, 'a': 55, 'b': 56, 'c': 57, 'd': 58, 'e': 59, 'f': 60, 'g': 61, 'h': 62, 'i': 63, 'j': 64, 'k': 65, 'l': 66, 'm': 67, 'n': 68, 'o': 69, 'p': 70, 'q': 71, 'r': 72, 's': 73, 't': 74, 'u': 75, 'v': 76, 'w': 77, 'x': 78, 'y': 79, 'z': 80, '}': 81, '\\ufeff': 82}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import hw3.charnn as charnn\n",
    "\n",
    "char_to_idx, idx_to_char = charnn.char_maps(corpus)\n",
    "print(char_to_idx)\n",
    "\n",
    "test.assertEqual(len(char_to_idx), len(idx_to_char))\n",
    "test.assertSequenceEqual(list(char_to_idx.keys()), list(idx_to_char.values()))\n",
    "test.assertSequenceEqual(list(char_to_idx.values()), list(idx_to_char.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems we have some strange characters in the corpus that are very rare and are probably due to mistakes.\n",
    "To reduce the length of each tensor we'll need to later represent our chars, it's best to remove them.\n",
    "\n",
    "**TODO**: Implement the `remove_chars()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Removed 34 chars\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "corpus, n_removed = charnn.remove_chars(corpus, ['}','$','_','<','\\ufeff'])\n",
    "print(f'Removed {n_removed} chars')\n",
    "\n",
    "# After removing the chars, re-create the mappings\n",
    "char_to_idx, idx_to_char = charnn.char_maps(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need is an **embedding** of the chracters.\n",
    "An embedding is a representation of each token from the sequence as a tensor.\n",
    "For a char-level RNN, our tokens will be chars and we can thus use the simplest possible embedding: encode each char as a **one-hot** tensor. In other words, each char will be represented\n",
    "as a tensor whos length is the total number of unique chars (`V`) which contains all zeros except at the index\n",
    "corresponding to that specific char.\n",
    "\n",
    "**TODO**: Implement the functions `chars_to_onehot()` and `onehot_to_chars()` in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "brine a maiden can season her praise in.\n",
      "   \n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]], dtype=torch.int8)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Wrap the actual embedding functions for calling convenience\n",
    "def embed(text):\n",
    "    return charnn.chars_to_onehot(text, char_to_idx)\n",
    "\n",
    "def unembed(embedding):\n",
    "    return charnn.onehot_to_chars(embedding, idx_to_char)\n",
    "\n",
    "text_snippet = corpus[3104:3148]\n",
    "print(text_snippet)\n",
    "print(embed(text_snippet[0:3]))\n",
    "\n",
    "test.assertEqual(text_snippet, unembed(embed(text_snippet)))\n",
    "test.assertEqual(embed(text_snippet).dtype, torch.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "<a id=part1_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to train our model to generate text by constantly predicting what the next char should be based on the past.\n",
    "To that end we'll need to train our recurrent network in a way similar to a classification task. At each timestep, we input a char and set the expected output (label) to be the next char in the original sequence.\n",
    "\n",
    "We will split our corpus into shorter sequences of length `S` chars (see question below).\n",
    "Each **sample** we provide our model with will therefore be a tensor of shape `(S,V)` where `V` is the embedding dimension. Our model will operate sequentially on each char in the sequence.\n",
    "For each sample, we'll also need a **label**. This is simply another sequence, shifted by one char so that the label of each char is the next char in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `chars_to_labelled_samples()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "samples shape: torch.Size([99182, 64, 78])\n",
      "labels shape: torch.Size([99182, 64])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Create dataset of sequences\n",
    "seq_len = 64\n",
    "vocab_len = len(char_to_idx)\n",
    "\n",
    "# Create labelled samples\n",
    "samples, labels = charnn.chars_to_labelled_samples(corpus, char_to_idx, seq_len, device)\n",
    "print(f'samples shape: {samples.shape}')\n",
    "print(f'labels shape: {labels.shape}')\n",
    "\n",
    "# Test shapes\n",
    "num_samples = (len(corpus) - 1) // seq_len\n",
    "test.assertEqual(samples.shape, (num_samples, seq_len, vocab_len))\n",
    "test.assertEqual(labels.shape, (num_samples, seq_len))\n",
    "\n",
    "# Test content\n",
    "for _ in range(1000):\n",
    "    # random sample\n",
    "    i = np.random.randint(num_samples, size=(1,))[0]\n",
    "    # Compare to corpus\n",
    "    test.assertEqual(unembed(samples[i]), corpus[i*seq_len:(i+1)*seq_len], msg=f\"content mismatch in sample {i}\")\n",
    "    # Compare to labels\n",
    "    sample_text = unembed(samples[i])\n",
    "    label_text = str.join('', [idx_to_char[j.item()] for j in labels[i]])\n",
    "    test.assertEqual(sample_text[1:], label_text[0:-1], msg=f\"label mismatch in sample {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a few consecutive samples. You should see that the text continues between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "sample [95177]:\n",
      "\tto stick it in their children's sight For terror, not to use, in\n",
      "sample [95178]:\n",
      "\ttime the rod Becomes more mock'd than fear'd; so our decrees, D\n",
      "sample [95179]:\n",
      "\tead to infliction, to themselves are dead; And liberty plucks ju\n",
      "sample [95180]:\n",
      "\tstice by the nose; The baby beats the nurse, and quite athwart G\n",
      "sample [95181]:\n",
      "\toes all decorum. FRIAR THOMAS: It rested in your grace To unloo\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "i = random.randrange(num_samples-5)\n",
    "for i in range(i, i+5):\n",
    "    s = re.sub(r'\\s+', ' ', unembed(samples[i])).strip()\n",
    "    print(f'sample [{i}]:\\n\\t{s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, instead of feeding one sample at a time into our model's forward we'll work with **batches** of samples. This means that at every timestep, our model will operate on a batch of chars that are from **different sequences**.\n",
    "Effectively this will allow us to parallelize training our model by dong matrix-matrix multiplications\n",
    "instead of matrix-vector during the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important nuance is that we need the batches to be **contiguous**, i.e. sample $k$ in batch $j$ should continue sample $k$ from batch $j-1$.\n",
    "The following figure illustrates this:\n",
    "\n",
    "<img src=\"imgs/rnn-batching.png\"/>\n",
    "\n",
    "If we na√Øvely take consecutive samples into batches, e.g. `[0,1,...,B-1]`, `[B,B+1,...,2B-1]` and so on, we won't have contiguous\n",
    "sequences at the same index between adjacent batches.\n",
    "\n",
    "To accomplish this we need to tell our `DataLoader` which samples to combine together into one batch.\n",
    "We do this by implementing a custom PyTorch `Sampler`, and providing it to our `DataLoader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `SequenceBatchSampler` class in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "sampler_idx =\n",
      " [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 2, 5, 8, 11, 14, 17, 20, 23, 26, 29]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from hw3.charnn import SequenceBatchSampler\n",
    "\n",
    "sampler = SequenceBatchSampler(dataset=range(32), batch_size=10)\n",
    "sampler_idx = list(sampler)\n",
    "print('sampler_idx =\\n', sampler_idx)\n",
    "\n",
    "# Test the Sampler\n",
    "test.assertEqual(len(sampler_idx), 30)\n",
    "batch_idx = np.array(sampler_idx).reshape(-1, 10)\n",
    "for k in range(10):\n",
    "    test.assertEqual(np.diff(batch_idx[:, k], n=2).item(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we're working with sequences, we can still use the standard PyTorch `Dataset`/`DataLoader` combo.\n",
    "For the dataset we can use a built-in class, `TensorDataset` to return tuples of `(sample, label)`\n",
    "from the `samples` and `labels` tensors we created above.\n",
    "The `DataLoader` will be provided with our custom `Sampler` so that it generates appropriate batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "# Create DataLoader returning batches of samples.\n",
    "batch_size = 32\n",
    "\n",
    "ds_corpus = torch.utils.data.TensorDataset(samples, labels)\n",
    "sampler_corpus = SequenceBatchSampler(ds_corpus, batch_size)\n",
    "dl_corpus = torch.utils.data.DataLoader(ds_corpus, batch_size=batch_size, sampler=sampler_corpus, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what that gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "num batches: 3100\n",
      "shape of a batch of samples: torch.Size([32, 64, 78])\n",
      "shape of a batch of labels: torch.Size([32, 64])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(f'num batches: {len(dl_corpus)}')\n",
    "\n",
    "x0, y0 = next(iter(dl_corpus))\n",
    "print(f'shape of a batch of samples: {x0.shape}')\n",
    "print(f'shape of a batch of labels: {y0.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the same sample index from multiple batches taken from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "=== batch 0, sample 18 (torch.Size([64, 78])): ===\n",
      "\tnstrous. Iago, who began't? MONTANO. If partially affined, or\n",
      "=== batch 1, sample 18 (torch.Size([64, 78])): ===\n",
      "\tleagued in office, Thou dost deliver more or less than truth\n",
      "=== batch 2, sample 18 (torch.Size([64, 78])): ===\n",
      "\t, Thou art no soldier. IAGO. Touch me not\n",
      "=== batch 3, sample 18 (torch.Size([64, 78])): ===\n",
      "\tso near: I had rather have this tongue cut from my mouth\n",
      "=== batch 4, sample 18 (torch.Size([64, 78])): ===\n",
      "\tThan it should do offense to Michael Cassio; Yet, I pers\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Check that sentences in in same index of different batches complete each other.\n",
    "k = random.randrange(batch_size)\n",
    "for j, (X, y) in enumerate(dl_corpus,):\n",
    "    print(f'=== batch {j}, sample {k} ({X[k].shape}): ===')\n",
    "    s = re.sub(r'\\s+', ' ', unembed(X[k])).strip()\n",
    "    print(f'\\t{s}')\n",
    "    if j==4: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "<a id=part1_5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our data set is ready so we can focus on our model.\n",
    "\n",
    "We'll implement here is a multilayer gated recurrent unit (GRU) model, with dropout.\n",
    "This model is a type of RNN which performs similar to the well-known LSTM model,\n",
    "but it's somewhat easier to train because it has less parameters.\n",
    "We'll modify the regular GRU slightly by applying dropout to\n",
    "the hidden states passed between layers of the model.\n",
    "\n",
    "The model accepts an input $\\mat{X}\\in\\set{R}^{S\\times V}$ containing a sequence of embedded chars.\n",
    "It returns an output $\\mat{Y}\\in\\set{R}^{S\\times V}$ of predictions for the next char and the final hidden state\n",
    "$\\mat{H}\\in\\set{R}^{L\\times H}$. Here $S$ is the sequence length, $V$ is the vocabulary size (number of unique chars), $L$ is the number of layers in the model and $H$ is the hidden dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the model's forward function at layer $k\\in[1,L]$ and timestep $t\\in[1,S]$ can be described as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{z_t}^{[k]} &= \\sigma\\left(\\vec{x}^{[k]}_t {\\mattr{W}_{\\mathrm{xz}}}^{[k]} +\n",
    "    \\vec{h}_{t-1}^{[k]} {\\mattr{W}_{\\mathrm{hz}}}^{[k]} + \\vec{b}_{\\mathrm{z}}^{[k]}\\right) \\\\\n",
    "\\vec{r_t}^{[k]} &= \\sigma\\left(\\vec{x}^{[k]}_t {\\mattr{W}_{\\mathrm{xr}}}^{[k]} +\n",
    "    \\vec{h}_{t-1}^{[k]} {\\mattr{W}_{\\mathrm{hr}}}^{[k]} + \\vec{b}_{\\mathrm{r}}^{[k]}\\right) \\\\\n",
    "\\vec{g_t}^{[k]} &= \\tanh\\left(\\vec{x}^{[k]}_t {\\mattr{W}_{\\mathrm{xg}}}^{[k]} +\n",
    "    (\\vec{r_t}^{[k]}\\odot\\vec{h}_{t-1}^{[k]}) {\\mattr{W}_{\\mathrm{hg}}}^{[k]} + \\vec{b}_{\\mathrm{g}}^{[k]}\\right) \\\\\n",
    "\\vec{h_t}^{[k]} &= \\vec{z}^{[k]}_t \\odot \\vec{h}^{[k]}_{t-1} + \\left(1-\\vec{z}^{[k]}_t\\right)\\odot \\vec{g_t}^{[k]}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to each layer is,\n",
    "$$\n",
    "\\mat{X}^{[k]} =\n",
    "\\begin{bmatrix}\n",
    "    {\\vec{x}_1}^{[k]} \\\\ \\vdots \\\\ {\\vec{x}_S}^{[k]}\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{cases}\n",
    "    \\mat{X} & \\mathrm{if} ~k = 1~ \\\\\n",
    "    \\mathrm{dropout}_p \\left(\n",
    "    \\begin{bmatrix}\n",
    "        {\\vec{h}_1}^{[k-1]} \\\\ \\vdots \\\\ {\\vec{h}_S}^{[k-1]}\n",
    "    \\end{bmatrix} \\right) & \\mathrm{if} ~1 < k \\leq L+1~\n",
    "\\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the entire model is then,\n",
    "$$\n",
    "\\mat{Y} = \\mat{X}^{[L+1]} {\\mattr{W}_{\\mathrm{hy}}} + \\mat{B}_{\\mathrm{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the final hidden state is\n",
    "$$\n",
    "\\mat{H} = \n",
    "\\begin{bmatrix}\n",
    "    {\\vec{h}_S}^{[1]} \\\\ \\vdots \\\\ {\\vec{h}_S}^{[L]}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- $t\\in[1,S]$ is the timestep, i.e. the current position within the sequence of each sample.\n",
    "- $\\vec{x}_t^{[k]}$ is the input of layer $k$ at timestep $t$, respectively.\n",
    "- The outputs of the **last layer** $\\vec{y}_t^{[L]}$, are the predicted next characters for every input char.\n",
    "  These are similar to class scores in classification tasks.\n",
    "- The hidden states at the **last timestep**, $\\vec{h}_S^{[k]}$, are the final hidden state returned from the model.\n",
    "- $\\sigma(\\cdot)$ is the sigmoid function, i.e. $\\sigma(\\vec{z}) = 1/(1+e^{-\\vec{z}})$ which returns values in $(0,1)$.\n",
    "- $\\tanh(\\cdot)$ is the hyperbolic tangent, i.e. $\\tanh(\\vec{z}) = (e^{2\\vec{z}}-1)/(e^{2\\vec{z}}+1)$ which returns values in $(-1,1)$.\n",
    "- $\\vec{h_t}^{[k]}$ is the hidden state of layer $k$ at time $t$. This can be thought of as the memory of that layer.\n",
    "- $\\vec{g_t}^{[k]}$ is the candidate hidden state for time $t+1$.\n",
    "- $\\vec{z_t}^{[k]}$ is known as the update gate. It combines the previous state with the input to determine how much the current state will be combined with the new candidate state. For example, if $\\vec{z_t}^{[k]}=\\vec{1}$ then the current input has no effect on the output.\n",
    "- $\\vec{r_t}^{[k]}$ is known as the reset gate. It combines the previous state with the input to determine how much of the previous state will affect the current state candidate. For example if $\\vec{r_t}^{[k]}=\\vec{0}$ the previous state has no effect on the current candidate state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a graphical representation of the GRU's forward pass at each timestep. The $\\vec{\\tilde{h}}$ in the image is our $\\vec{g}$ (candidate next state).\n",
    "\n",
    "<img src=\"imgs/gru_cell.png\" width=\"400\"/>\n",
    "\n",
    "You can see how the reset and update gates allow the model to completely ignore it's previous state, completely ignore it's input, or any mixture of those states (since the gates are actually continuous and between $(0,1)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a graphical representation of the entire model.\n",
    "You can ignore the $c_t^{[k]}$ (cell state) variables (which are relevant for LSTM models).\n",
    "Our model has only the hidden state, $h_t^{[k]}$. Also notice that we added dropout between layers (i.e., on the up arrows).\n",
    "\n",
    "<img src=\"imgs/lstm_model.png\" />\n",
    "\n",
    "The purple tensors are inputs (a sequence and initial hidden state per layer), and the green tensors are outputs (another sequence and final hidden state per layer). Each blue block implements the above forward equations.\n",
    "Blocks that are on the same vertical level are at the same layer, and therefore share parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `MultilayerGRU` class in the `hw3/charnn.py` module.\n",
    "\n",
    "Notes:\n",
    "- You'll need to handle input **batches** now.\n",
    "  The math is identical to the above, but all the tensors will have an extra batch\n",
    "  dimension as their first dimension.\n",
    "- Use the diagram above to help guide your implementation.\n",
    "  It will help you visualize what shapes to returns where, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "MultilayerGRU(\n",
      "  (w_xz_0): Linear(in_features=78, out_features=256, bias=False)\n",
      "  (w_hz_0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (w_xr_0): Linear(in_features=78, out_features=256, bias=False)\n",
      "  (w_hr_0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (w_xg_0): Linear(in_features=78, out_features=256, bias=False)\n",
      "  (w_hg_0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (w_xz_1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (w_hz_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (w_xr_1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (w_hr_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (w_xg_1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (w_hg_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (w_xz_2): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (w_hz_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (w_xr_2): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (w_hr_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (w_xg_2): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (w_hg_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (out): Linear(in_features=256, out_features=78, bias=True)\n",
      ")\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-61ccf2c601fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Test forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'y.shape={y.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'h.shape={h.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs236781-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\cs236781-HW3\\hw3\\charnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m                 \u001b[0mnext_layer_inputs_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;31m#pre_layer_input = torch.FloatTensor(next_layer_inputs_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[0mlayer_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_layer_inputs_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m             \u001b[0mlayer_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\cs236781-HW3\\hw3\\charnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m                 \u001b[0mnext_layer_inputs_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;31m#pre_layer_input = torch.FloatTensor(next_layer_inputs_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[0mlayer_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_layer_inputs_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m             \u001b[0mlayer_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\JetBrains\\PyCharm 2019.3.3\\plugins\\python\\helpers\\pydev\\pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_wait_suspend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\JetBrains\\PyCharm 2019.3.3\\plugins\\python\\helpers\\pydev\\pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "in_dim = vocab_len\n",
    "h_dim = 256\n",
    "n_layers = 3\n",
    "model = charnn.MultilayerGRU(in_dim, h_dim, out_dim=in_dim, n_layers=n_layers)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "y, h = model(x0.to(dtype=torch.float))\n",
    "print(f'y.shape={y.shape}')\n",
    "print(f'h.shape={h.shape}')\n",
    "\n",
    "test.assertEqual(y.shape, (batch_size, seq_len, vocab_len))\n",
    "test.assertEqual(h.shape, (batch_size, n_layers, h_dim))\n",
    "test.assertEqual(len(list(model.parameters())), 9 * n_layers + 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text by sampling\n",
    "<a id=part1_6></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model, we can implement **text generation** based on it.\n",
    "The idea is simple:\n",
    "At each timestep our model receives one char $x_t$ from the input sequence and outputs scores $y_t$\n",
    "for what the next char should be.\n",
    "We'll convert these scores into a probability over each of the possible chars.\n",
    "In other words, for each input char $x_t$ we create a probability distribution for the next char\n",
    "conditioned on the current one and the state of the model (representing all previous inputs):\n",
    "$$p(x_{t+1}|x_t, \\vec{h}_t).$$\n",
    "\n",
    "Once we have such a distribution, we'll sample a char from it.\n",
    "This will be the first char of our generated sequence.\n",
    "Now we can feed this new char into the model, create another distribution, sample the next char and so on.\n",
    "Note that it's crucial to propagate the hidden state when sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important point however is how to create the distribution from the scores.\n",
    "One way, as we saw in previous ML tasks, is to use the softmax function.\n",
    "However, a drawback of softmax is that it can generate very diffuse (more uniform) distributions if the score values are very similar. When sampling, we would prefer to control the distributions and make them less uniform to increase the chance of sampling the char(s) with the highest scores compared to the others.\n",
    "\n",
    "To control the variance of the distribution, a common trick is to add a hyperparameter $T$, known as the \n",
    "*temperature* to the softmax function. The class scores are simply scaled by $T$ before softmax is applied:\n",
    "$$\n",
    "\\mathrm{softmax}_T(\\vec{y}) = \\frac{e^{\\vec{y}/T}}{\\sum_k e^{y_k/T}}\n",
    "$$\n",
    "\n",
    "A low $T$ will result in less uniform distributions and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `hot_softmax()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-95afe7c79070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcharnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhot_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'T={t}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'$x_{t+1}$'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'$p(x_{t+1}|x_t)$'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\cs236781-HW3\\hw3\\charnn.py\u001b[0m in \u001b[0;36mhot_softmax\u001b[1;34m(y, dim, temperature)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;31m# TODO: Implement based on the above.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;31m# ====== YOUR CODE: ======\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;31m# ========================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ],
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error"
    },
    {
     "data": {
      "text/plain": "<Figure size 1080x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = y[0,0,:].detach()\n",
    "_, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "for t in reversed([0.3, 0.5, 1.0, 100]):\n",
    "    ax.plot(charnn.hot_softmax(scores, temperature=t).cpu().numpy(), label=f'T={t}')\n",
    "ax.set_xlabel('$x_{t+1}$')\n",
    "ax.set_ylabel('$p(x_{t+1}|x_t)$')\n",
    "ax.legend()\n",
    "\n",
    "uniform_proba = 1/len(char_to_idx)\n",
    "uniform_diff = torch.abs(charnn.hot_softmax(scores, temperature=100) - uniform_proba)\n",
    "test.assertTrue(torch.all(uniform_diff < 1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `generate_from_model()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    text = charnn.generate_from_model(model, \"foobar\", 50, (char_to_idx, idx_to_char), T=0.5)\n",
    "    print(text)\n",
    "    test.assertEqual(len(text), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "<a id=part1_7></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model, we'll calculate the loss at each time step by comparing the predicted char to\n",
    "the actual char from our label. We can use cross entropy since per char it's similar to a classification problem.\n",
    "We'll then sum the losses over the sequence and back-propagate the gradients though time.\n",
    "Notice that the back-propagation algorithm will \"visit\" each layer's parameter tensors multiple times,\n",
    "so we'll accumulate gradients in parameters of the blocks. Luckily `autograd` will handle this part for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the first step of training will be to try and **overfit** a large model (many parameters) to a tiny dataset.\n",
    "Again, this is to ensure the model and training code are implemented correctly, i.e. that the model can learn.\n",
    "\n",
    "For a generative model such as this, overfitting is slightly trickier than for for classification.\n",
    "What we'll aim to do is to get our model to **memorize** a specific sequence of chars, so that when given the first\n",
    "char in the sequence it will immediately spit out the rest of the sequence verbatim.\n",
    "\n",
    "Let's create a tiny dataset to memorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Pick a tiny subset of the dataset\n",
    "subset_start, subset_end = 1001, 1005\n",
    "ds_corpus_ss = torch.utils.data.Subset(ds_corpus, range(subset_start, subset_end))\n",
    "batch_size_ss = 1\n",
    "sampler_ss = SequenceBatchSampler(ds_corpus_ss, batch_size=batch_size_ss)\n",
    "dl_corpus_ss = torch.utils.data.DataLoader(ds_corpus_ss, batch_size_ss, sampler=sampler_ss, shuffle=False)\n",
    "\n",
    "# Convert subset to text\n",
    "subset_text = ''\n",
    "for i in range(subset_end - subset_start):\n",
    "    subset_text += unembed(ds_corpus_ss[i][0])\n",
    "print(f'Text to \"memorize\":\\n\\n{subset_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the first part of our training code.\n",
    "\n",
    "**TODO**: Implement the `train_epoch()` and `train_batch()` methods of the `RNNTrainer` class in the `hw3/training.py` module. \n",
    "You must think about how to correctly handle the hidden state of the model between batches and epochs for this specific task (i.e. text generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from hw3.training import RNNTrainer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.01\n",
    "num_epochs = 500\n",
    "\n",
    "in_dim = vocab_len\n",
    "h_dim = 128\n",
    "n_layers = 2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = charnn.MultilayerGRU(in_dim, h_dim, out_dim=in_dim, n_layers=n_layers).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "trainer = RNNTrainer(model, loss_fn, optimizer, device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_result = trainer.train_epoch(dl_corpus_ss, verbose=False)\n",
    "    \n",
    "    # Every X epochs, we'll generate a sequence starting from the first char in the first sequence\n",
    "    # to visualize how/if/what the model is learning.\n",
    "    if epoch == 0 or (epoch+1) % 25 == 0:\n",
    "        avg_loss = np.mean(epoch_result.losses)\n",
    "        accuracy = np.mean(epoch_result.accuracy)\n",
    "        print(f'\\nEpoch #{epoch+1}: Avg. loss = {avg_loss:.3f}, Accuracy = {accuracy:.2f}%')\n",
    "        \n",
    "        generated_sequence = charnn.generate_from_model(model, subset_text[0],\n",
    "                                                        seq_len*(subset_end-subset_start),\n",
    "                                                        (char_to_idx,idx_to_char), T=0.1)\n",
    "        \n",
    "        # Stop if we've successfully memorized the small dataset.\n",
    "        print(generated_sequence)\n",
    "        if generated_sequence == subset_text:\n",
    "            break\n",
    "\n",
    "# Test successful overfitting\n",
    "test.assertGreater(epoch_result.accuracy, 99)\n",
    "test.assertEqual(generated_sequence, subset_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so training works - we can memorize a short sequence.\n",
    "We'll now train a much larger model on our large dataset. You'll need a GPU for this part.\n",
    "\n",
    "First, lets set up our dataset and models for training.\n",
    "We'll split our corpus into 90% train and 10% test-set.\n",
    "Also, we'll use a learning-rate scheduler to control the learning rate during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Set the hyperparameters in the `part1_rnn_hyperparams()` function of the `hw3/answers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from hw3.answers import part1_rnn_hyperparams\n",
    "\n",
    "hp = part1_rnn_hyperparams()\n",
    "print('hyperparams:\\n', hp)\n",
    "\n",
    "### Dataset definition\n",
    "vocab_len = len(char_to_idx)\n",
    "batch_size = hp['batch_size']\n",
    "seq_len = hp['seq_len']\n",
    "train_test_ratio = 0.9\n",
    "num_samples = (len(corpus) - 1) // seq_len\n",
    "num_train = int(train_test_ratio * num_samples)\n",
    "\n",
    "samples, labels = charnn.chars_to_labelled_samples(corpus, char_to_idx, seq_len, device)\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(samples[:num_train], labels[:num_train])\n",
    "sampler_train = SequenceBatchSampler(ds_train, batch_size)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=False, sampler=sampler_train, drop_last=True)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(samples[num_train:], labels[num_train:])\n",
    "sampler_test = SequenceBatchSampler(ds_test, batch_size)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=False, sampler=sampler_test, drop_last=True)\n",
    "\n",
    "print(f'Train: {len(dl_train):3d} batches, {len(dl_train)*batch_size*seq_len:7d} chars')\n",
    "print(f'Test:  {len(dl_test):3d} batches, {len(dl_test)*batch_size*seq_len:7d} chars')\n",
    "\n",
    "### Training definition\n",
    "in_dim = out_dim = vocab_len\n",
    "checkpoint_file = 'checkpoints/rnn'\n",
    "num_epochs = 50\n",
    "early_stopping = 5\n",
    "\n",
    "model = charnn.MultilayerGRU(in_dim, hp['h_dim'], out_dim, hp['n_layers'], hp['dropout'])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hp['learn_rate'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=hp['lr_sched_factor'], patience=hp['lr_sched_patience'], verbose=True\n",
    ")\n",
    "trainer = RNNTrainer(model, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code blocks below will train the model and save checkpoints containing the training state and the best model parameters to a file. This allows you to stop training and resume it later from where you left.\n",
    "\n",
    "Note that you can use the `main.py` script provided within the assignment folder to run this notebook from the command line as if it were a python script by using the `run-nb` subcommand. This allows you to train your model using this notebook without starting jupyter. You can combine this with `srun` or `sbatch` to run the notebook with a GPU on the course servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "- Implement the `fit()` method of the `Trainer` class. You can reuse the relevant implementation parts from HW2, but make sure to implement early stopping and checkpoints.\n",
    "- Implement the `test_epoch()` and `test_batch()` methods of the `RNNTrainer` class in the `hw3/training.py` module.\n",
    "- Run the following block to train.\n",
    "- When training is done and you're satisfied with the model's outputs, rename the checkpoint file to `checkpoints/rnn_final.pt`.\n",
    "  This will cause the block to skip training and instead load your saved model when running the homework submission script.\n",
    "  Note that your submission zip file will not include the checkpoint file. This is OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from cs236781.plot import plot_fit\n",
    "\n",
    "def post_epoch_fn(epoch, train_res, test_res, verbose):\n",
    "    # Update learning rate\n",
    "    scheduler.step(test_res.accuracy)\n",
    "    # Sample from model to show progress\n",
    "    if verbose:\n",
    "        start_seq = \"ACT I.\"\n",
    "        generated_sequence = charnn.generate_from_model(\n",
    "            model, start_seq, 100, (char_to_idx,idx_to_char), T=0.5\n",
    "        )\n",
    "        print(generated_sequence)\n",
    "\n",
    "# Train, unless final checkpoint is found\n",
    "checkpoint_file_final = f'{checkpoint_file}_final.pt'\n",
    "if os.path.isfile(checkpoint_file_final):\n",
    "    print(f'*** Loading final checkpoint file {checkpoint_file_final} instead of training')\n",
    "    saved_state = torch.load(checkpoint_file_final, map_location=device)\n",
    "    model.load_state_dict(saved_state['model_state'])\n",
    "else:\n",
    "    try:\n",
    "        # Print pre-training sampling\n",
    "        print(charnn.generate_from_model(model, \"ACT I.\", 100, (char_to_idx,idx_to_char), T=0.5))\n",
    "\n",
    "        fit_res = trainer.fit(dl_train, dl_test, num_epochs, max_batches=None,\n",
    "                              post_epoch_fn=post_epoch_fn, early_stopping=early_stopping,\n",
    "                              checkpoints=checkpoint_file, print_every=1)\n",
    "        \n",
    "        fig, axes = plot_fit(fit_res)\n",
    "    except KeyboardInterrupt as e:\n",
    "        print('\\n *** Training interrupted by user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a work of art\n",
    "<a id=part1_8></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with our fully trained model, let's generate the next Hamlet! You should experiment with modifying the sampling temperature and see what happens.\n",
    "\n",
    "The text you generate should ‚Äúlook‚Äù like a Shakespeare play:\n",
    "old-style English words and sentence structure, directions for the actors\n",
    "(like ‚ÄúExit/Enter‚Äù), sections (Act I/Scene III) etc.\n",
    "There will be no coherent plot of course, but it should at least seem like\n",
    "a Shakespearean play when not looking too closely.\n",
    "If this is not what you see, go back, debug and/or and re-train.\n",
    "\n",
    "**TODO**: Specify the generation parameters in the `part1_generation_params()` function within the `hw3/answers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from hw3.answers import part1_generation_params\n",
    "\n",
    "start_seq, temperature = part1_generation_params()\n",
    "\n",
    "generated_sequence = charnn.generate_from_model(\n",
    "    model, start_seq, 10000, (char_to_idx,idx_to_char), T=temperature\n",
    ")\n",
    "\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "<a id=part1_9></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw3/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw3.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Why do we split the corpus into sequences instead of training on the whole text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part1_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "How is it possible that the generated text clearly shows memory longer than the sequence length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part1_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why are we not shuffling the order of batches when training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part1_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "1. Why do we lower the temperature for sampling (compared to the default of $1.0$ when training)?\n",
    "2. What happens when the temperature is very high and why?\n",
    "3. What happens when the temperature is very low and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part1_q4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}